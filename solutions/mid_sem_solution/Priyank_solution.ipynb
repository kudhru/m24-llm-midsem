{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ExtractionState' object has no attribute 'class_evaluations'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[111], line 175\u001B[0m\n\u001B[1;32m    172\u001B[0m class_extraction_graph \u001B[38;5;241m=\u001B[39m graph\u001B[38;5;241m.\u001B[39mcompile(checkpointer\u001B[38;5;241m=\u001B[39mmemory)\n\u001B[1;32m    174\u001B[0m \u001B[38;5;66;03m# Execute the graph and save results\u001B[39;00m\n\u001B[0;32m--> 175\u001B[0m \u001B[38;5;28;01mawait\u001B[39;00m class_extraction_graph\u001B[38;5;241m.\u001B[39mainvoke(state, config\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconfigurable\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthread_id\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mstr\u001B[39m(uuid\u001B[38;5;241m.\u001B[39muuid4())}})\n\u001B[1;32m    176\u001B[0m save_final_evaluations(state)\n",
      "File \u001B[0;32m~/Downloads/LG1/venv/lib/python3.9/site-packages/langgraph/pregel/__init__.py:1613\u001B[0m, in \u001B[0;36mPregel.ainvoke\u001B[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001B[0m\n\u001B[1;32m   1611\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1612\u001B[0m     chunks \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m-> 1613\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mastream(\n\u001B[1;32m   1614\u001B[0m     \u001B[38;5;28minput\u001B[39m,\n\u001B[1;32m   1615\u001B[0m     config,\n\u001B[1;32m   1616\u001B[0m     stream_mode\u001B[38;5;241m=\u001B[39mstream_mode,\n\u001B[1;32m   1617\u001B[0m     output_keys\u001B[38;5;241m=\u001B[39moutput_keys,\n\u001B[1;32m   1618\u001B[0m     interrupt_before\u001B[38;5;241m=\u001B[39minterrupt_before,\n\u001B[1;32m   1619\u001B[0m     interrupt_after\u001B[38;5;241m=\u001B[39minterrupt_after,\n\u001B[1;32m   1620\u001B[0m     debug\u001B[38;5;241m=\u001B[39mdebug,\n\u001B[1;32m   1621\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   1622\u001B[0m ):\n\u001B[1;32m   1623\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m stream_mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalues\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1624\u001B[0m         latest \u001B[38;5;241m=\u001B[39m chunk\n",
      "File \u001B[0;32m~/Downloads/LG1/venv/lib/python3.9/site-packages/langgraph/pregel/__init__.py:1502\u001B[0m, in \u001B[0;36mPregel.astream\u001B[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001B[0m\n\u001B[1;32m   1491\u001B[0m \u001B[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001B[39;00m\n\u001B[1;32m   1492\u001B[0m \u001B[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001B[39;00m\n\u001B[1;32m   1493\u001B[0m \u001B[38;5;66;03m# channel updates from step N are only visible in step N+1\u001B[39;00m\n\u001B[1;32m   1494\u001B[0m \u001B[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001B[39;00m\n\u001B[1;32m   1495\u001B[0m \u001B[38;5;66;03m# with channel updates applied only at the transition between steps\u001B[39;00m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m loop\u001B[38;5;241m.\u001B[39mtick(\n\u001B[1;32m   1497\u001B[0m     input_keys\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_channels,\n\u001B[1;32m   1498\u001B[0m     interrupt_before\u001B[38;5;241m=\u001B[39minterrupt_before_,\n\u001B[1;32m   1499\u001B[0m     interrupt_after\u001B[38;5;241m=\u001B[39minterrupt_after_,\n\u001B[1;32m   1500\u001B[0m     manager\u001B[38;5;241m=\u001B[39mrun_manager,\n\u001B[1;32m   1501\u001B[0m ):\n\u001B[0;32m-> 1502\u001B[0m     \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m runner\u001B[38;5;241m.\u001B[39matick(\n\u001B[1;32m   1503\u001B[0m         loop\u001B[38;5;241m.\u001B[39mtasks\u001B[38;5;241m.\u001B[39mvalues(),\n\u001B[1;32m   1504\u001B[0m         timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstep_timeout,\n\u001B[1;32m   1505\u001B[0m         retry_policy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mretry_policy,\n\u001B[1;32m   1506\u001B[0m         get_waiter\u001B[38;5;241m=\u001B[39mget_waiter,\n\u001B[1;32m   1507\u001B[0m     ):\n\u001B[1;32m   1508\u001B[0m         \u001B[38;5;66;03m# emit output\u001B[39;00m\n\u001B[1;32m   1509\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m o \u001B[38;5;129;01min\u001B[39;00m output():\n\u001B[1;32m   1510\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m o\n",
      "File \u001B[0;32m~/Downloads/LG1/venv/lib/python3.9/site-packages/langgraph/pregel/runner.py:130\u001B[0m, in \u001B[0;36mPregelRunner.atick\u001B[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001B[0m\n\u001B[1;32m    128\u001B[0m t \u001B[38;5;241m=\u001B[39m tasks[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    129\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 130\u001B[0m     \u001B[38;5;28;01mawait\u001B[39;00m arun_with_retry(t, retry_policy, stream\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_astream)\n\u001B[1;32m    131\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommit(t, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    132\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
      "File \u001B[0;32m~/Downloads/LG1/venv/lib/python3.9/site-packages/langgraph/pregel/retry.py:102\u001B[0m, in \u001B[0;36marun_with_retry\u001B[0;34m(task, retry_policy, stream)\u001B[0m\n\u001B[1;32m    100\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m    101\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 102\u001B[0m     \u001B[38;5;28;01mawait\u001B[39;00m task\u001B[38;5;241m.\u001B[39mproc\u001B[38;5;241m.\u001B[39mainvoke(task\u001B[38;5;241m.\u001B[39minput, config)\n\u001B[1;32m    103\u001B[0m \u001B[38;5;66;03m# if successful, end\u001B[39;00m\n\u001B[1;32m    104\u001B[0m \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/Downloads/LG1/venv/lib/python3.9/site-packages/langgraph/utils/runnable.py:454\u001B[0m, in \u001B[0;36mRunnableSeq.ainvoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    452\u001B[0m             \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m asyncio\u001B[38;5;241m.\u001B[39mcreate_task(coro, context\u001B[38;5;241m=\u001B[39mcontext)\n\u001B[1;32m    453\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 454\u001B[0m             \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m asyncio\u001B[38;5;241m.\u001B[39mcreate_task(coro)\n\u001B[1;32m    455\u001B[0m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n\u001B[1;32m    456\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/Downloads/LG1/venv/lib/python3.9/site-packages/langgraph/utils/runnable.py:237\u001B[0m, in \u001B[0;36mRunnableCallable.ainvoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    235\u001B[0m         ret \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m asyncio\u001B[38;5;241m.\u001B[39mcreate_task(coro, context\u001B[38;5;241m=\u001B[39mcontext)\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 237\u001B[0m         ret \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mafunc(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    238\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(ret, Runnable) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrecurse:\n\u001B[1;32m    239\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m ret\u001B[38;5;241m.\u001B[39mainvoke(\u001B[38;5;28minput\u001B[39m, config)\n",
      "Cell \u001B[0;32mIn[111], line 85\u001B[0m, in \u001B[0;36mevaluate_classes\u001B[0;34m(state)\u001B[0m\n\u001B[1;32m     78\u001B[0m prompt \u001B[38;5;241m=\u001B[39m generate_evaluation_prompt(\n\u001B[1;32m     79\u001B[0m     class_name, \n\u001B[1;32m     80\u001B[0m     student_class_code, \n\u001B[1;32m     81\u001B[0m     model_class_code, \n\u001B[1;32m     82\u001B[0m     rubric_criteria\n\u001B[1;32m     83\u001B[0m )\n\u001B[1;32m     84\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m llm\u001B[38;5;241m.\u001B[39mainvoke([HumanMessage(content\u001B[38;5;241m=\u001B[39mprompt)])\n\u001B[0;32m---> 85\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m state\u001B[38;5;241m.\u001B[39mclass_evaluations \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     86\u001B[0m     state\u001B[38;5;241m.\u001B[39mclass_evaluations \u001B[38;5;241m=\u001B[39m {}  \u001B[38;5;66;03m# Initialize if not already done\u001B[39;00m\n\u001B[1;32m     87\u001B[0m state\u001B[38;5;241m.\u001B[39mclass_evaluations[class_name] \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mcontent  \u001B[38;5;66;03m# Ensure class_evaluations is initialized before this line\u001B[39;00m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'ExtractionState' object has no attribute 'class_evaluations'"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "import json\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Initialize the OpenAI LLM model\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Define the state object to store data during the workflow\n",
    "class ExtractionState:\n",
    "    def __init__(self, student_file_content: str, model_file_content: str, rubric_content: str, question_content: str = None):\n",
    "        self.student_file_content = student_file_content\n",
    "        self.model_file_content = model_file_content\n",
    "        self.rubric_content = rubric_content\n",
    "        self.question_content = question_content  # This is now optional\n",
    "        self.student_classes = {}\n",
    "        self.model_classes = {}\n",
    "        self.class_rubric_mapping = {}\n",
    "        self.class_evaluations = {}  # Ensure this is initialized as an empty dictionary\n",
    "        self.final_class_evaluations = {}\n",
    "        self.total_marks = None\n",
    "\n",
    "# Class Extraction Module\n",
    "async def extract_student_classes(state: ExtractionState) -> ExtractionState:\n",
    "    prompt = f\"Extract the entire Java classes from the student's solution. Return a dictionary where the key is the class name, and the value is the entire class code:\\n{state.student_file_content}\"\n",
    "    response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    # Assuming the response is a JSON string, we parse it into a dictionary\n",
    "    try:\n",
    "        state.student_classes = json.loads(response.content)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Failed to parse student classes: {e}\")\n",
    "        state.student_classes = {}\n",
    "\n",
    "    return state\n",
    "\n",
    "async def extract_model_classes(state: ExtractionState) -> ExtractionState:\n",
    "    prompt = f\"Extract the entire Java classes from the model solution. Return a dictionary where the key is the class name, and the value is the entire class code:\\n{state.model_file_content}\"\n",
    "    response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    try:\n",
    "        state.model_classes = json.loads(response.content)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Failed to parse model classes: {e}\")\n",
    "        state.model_classes = {}\n",
    "\n",
    "    return state\n",
    "\n",
    "# Rubric Extraction Module\n",
    "async def extract_rubric_for_classes(state: ExtractionState) -> ExtractionState:\n",
    "    prompt = f\"Extract the relevant rubric for evaluating the following Java classes based on the provided rubric. Return a JSON where the key is the class name and the value is the corresponding rubric criteria:\\n{state.rubric_content}\"\n",
    "    response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    # Parse the response as a dictionary\n",
    "    try:\n",
    "        state.class_rubric_mapping = json.loads(response.content)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Failed to parse class rubric mapping: {e}\")\n",
    "        state.class_rubric_mapping = {}\n",
    "\n",
    "    return state\n",
    "\n",
    "# Initial Evaluation Module\n",
    "def generate_evaluation_prompt(class_name: str, student_class_code: str, model_class_code: str, rubric_criteria: list) -> str:\n",
    "    return (\n",
    "        f\"Evaluate the student's Java class '{class_name}' against the model class and rubric. \"\n",
    "        f\"**Student Class Code:**\\n{student_class_code}\\n\\n**Model Class Code:**\\n{model_class_code}\\n\\n\"\n",
    "        f\"**Rubric Criteria:**\\n{rubric_criteria}\\n\\n\"\n",
    "        \"Provide a detailed score for each criterion and specific feedback.\"\n",
    "    )\n",
    "\n",
    "async def evaluate_classes(state: ExtractionState) -> ExtractionState:\n",
    "    for class_name, student_class_code in state.student_classes.items():\n",
    "        model_class_code = state.model_classes.get(class_name, \"\")\n",
    "        rubric_criteria = state.class_rubric_mapping.get(class_name, [])\n",
    "        prompt = generate_evaluation_prompt(\n",
    "            class_name,\n",
    "            student_class_code,\n",
    "            model_class_code,\n",
    "            rubric_criteria\n",
    "        )\n",
    "        response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
    "        if state.class_evaluations is None:\n",
    "            state.class_evaluations = {}  # Initialize if not already done\n",
    "        state.class_evaluations[class_name] = response.content  # Ensure class_evaluations is initialized before this line\n",
    "    return state\n",
    "\n",
    "# Review Evaluation Module\n",
    "def generate_review_prompt(class_name: str, student_class_code: str, model_class_code: str, rubric_criteria: list, initial_evaluation: dict) -> str:\n",
    "    return (\n",
    "        f\"Review the initial evaluation of the student's class '{class_name}' and the provided feedback. \"\n",
    "        f\"**Student Class Code:**\\n{student_class_code}\\n\\n**Model Class Code:**\\n{model_class_code}\\n\"\n",
    "        f\"**Rubric Criteria:**\\n{rubric_criteria}\\n\\n\"\n",
    "        f\"**Initial Evaluation:**\\n{initial_evaluation}\\n\\n\"\n",
    "        \"Make necessary corrections, and provide the final assessment with feedback.\"\n",
    "    )\n",
    "\n",
    "async def review_evaluations(state: ExtractionState) -> ExtractionState:\n",
    "    for class_name, initial_evaluation in state.class_evaluations.items():\n",
    "        student_class_code = state.student_classes.get(class_name, \"\")\n",
    "        model_class_code = state.model_classes.get(class_name, \"\")\n",
    "        rubric_criteria = state.class_rubric_mapping.get(class_name, [])\n",
    "        prompt = generate_review_prompt(\n",
    "            class_name,\n",
    "            student_class_code,\n",
    "            model_class_code,\n",
    "            rubric_criteria,\n",
    "            initial_evaluation\n",
    "        )\n",
    "        response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
    "        state.final_class_evaluations[class_name] = response.content\n",
    "    return state\n",
    "\n",
    "# Marks Extraction Module\n",
    "def extract_marks(state: ExtractionState) -> ExtractionState:\n",
    "    for class_name, evaluation in state.final_class_evaluations.items():\n",
    "        marks_list = [str(crit.get(\"score\", 0)) for crit in evaluation.get(\"criterion_evaluations\", [])]\n",
    "        state.final_class_evaluations[class_name][\"marks\"] = \", \".join(marks_list)\n",
    "    return state\n",
    "\n",
    "# Total Marks Calculation Module\n",
    "@tool\n",
    "def sum_marks(marks_list: str) -> int:\n",
    "    \"\"\"\n",
    "    Takes a comma-separated list of marks and returns their sum.\n",
    "    Each mark is expected to be a valid integer.\n",
    "    \"\"\"\n",
    "    marks = [int(mark.strip()) for mark in marks_list.split(\",\")]\n",
    "    return sum(marks)\n",
    "\n",
    "async def calculate_total_marks(state: ExtractionState) -> ExtractionState:\n",
    "    total_marks_list = [evaluation.get(\"marks\", \"\") for evaluation in state.final_class_evaluations.values()]\n",
    "    combined_marks_list = \", \".join(total_marks_list)\n",
    "    llm.bind_tools([sum_marks])\n",
    "    prompt = f\"Calculate the total marks from the following list of marks: {combined_marks_list}\"\n",
    "    response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
    "    state.total_marks = response.content\n",
    "    return state\n",
    "\n",
    "# Save Final Evaluations to a File\n",
    "def save_final_evaluations(state: ExtractionState, filename=\"final_evaluations.txt\"):\n",
    "    with open(filename, \"w\") as file:\n",
    "        for class_name, evaluation in state.final_class_evaluations.items():\n",
    "            file.write(f\"Class: {class_name}\\n\")\n",
    "            file.write(json.dumps(evaluation, indent=2))\n",
    "            file.write(\"\\n\\n\")\n",
    "        file.write(f\"Total Marks: {state.total_marks}\\n\")\n",
    "    print(f\"Final evaluations and total marks saved to {filename}\")\n",
    "\n",
    "# LangGraph Workflow Construction\n",
    "graph = StateGraph(ExtractionState)\n",
    "graph.add_node(\"extract_student_classes\", extract_student_classes)\n",
    "graph.add_node(\"extract_model_classes\", extract_model_classes)\n",
    "graph.add_node(\"extract_rubric\", extract_rubric_for_classes)\n",
    "graph.add_node(\"evaluate_classes\", evaluate_classes)\n",
    "graph.add_node(\"review_evaluations\", review_evaluations)\n",
    "graph.add_node(\"extract_marks\", extract_marks)\n",
    "graph.add_node(\"calculate_total_marks\", calculate_total_marks)\n",
    "\n",
    "graph.add_edge(START, \"extract_student_classes\")\n",
    "graph.add_edge(\"extract_student_classes\", \"extract_model_classes\")\n",
    "graph.add_edge(\"extract_model_classes\", \"extract_rubric\")\n",
    "graph.add_edge(\"extract_rubric\", \"evaluate_classes\")\n",
    "graph.add_edge(\"evaluate_classes\", \"review_evaluations\")\n",
    "graph.add_edge(\"review_evaluations\", \"extract_marks\")\n",
    "graph.add_edge(\"extract_marks\", \"calculate_total_marks\")\n",
    "graph.add_edge(\"calculate_total_marks\", END)\n",
    "\n",
    "memory = MemorySaver()\n",
    "class_extraction_graph = graph.compile(checkpointer=memory)\n",
    "\n",
    "# Execute the graph and save results\n",
    "await class_extraction_graph.ainvoke(state, config={\"configurable\": {\"thread_id\": str(uuid.uuid4())}})\n",
    "save_final_evaluations(state)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
